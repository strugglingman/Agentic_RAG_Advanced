# Task 2: Redis + PostgreSQL for Conversation History

**Goal**: Implement hybrid storage for conversation history (Redis cache + PostgreSQL persistence)

**Time Estimate**: 6-8 hours
**Difficulty**: Medium-Hard
**Dependencies**: PostgreSQL, Redis, Prisma

---

## ğŸ“‹ **Table of Contents**

1. [Overview](#overview)
2. [Why Hybrid Storage?](#why-hybrid-storage)
3. [Architecture Design](#architecture-design)
4. [Prerequisites](#prerequisites)
5. [Database Schema Design](#database-schema-design)
6. [Implementation Plan](#implementation-plan)
7. [Migration Strategy](#migration-strategy)
8. [Testing Strategy](#testing-strategy)
9. [Success Criteria](#success-criteria)
10. [Troubleshooting](#troubleshooting)

---

## 1. Overview

### **Current State** (In-Memory)

```python
# backend/src/routes/chat.py line 21
SESSIONS = defaultdict(lambda: deque(maxlen=2 * Config.MAX_HISTORY))

# Problem:
# âŒ Lost on Flask restart
# âŒ Lost on deployment
# âŒ Can't scale to multiple servers
# âŒ No analytics possible
# âŒ Users lose history if session expires
```

---

### **Target State** (Hybrid Storage)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Request                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Flask App   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis (Cache)   â”‚          â”‚  PostgreSQL (DB) â”‚
â”‚  - Active        â”‚ â†syncâ†’   â”‚  - Permanent     â”‚
â”‚    sessions      â”‚          â”‚    storage       â”‚
â”‚  - Fast reads    â”‚          â”‚  - All history   â”‚
â”‚  - 1 hour TTL    â”‚          â”‚  - Analytics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flow:
1. Read: Check Redis â†’ Hit? Return (fast)
                    â†’ Miss? Load from PostgreSQL â†’ Cache in Redis
2. Write: Save to PostgreSQL â†’ Update Redis cache
3. Expire: Redis auto-deletes after 1 hour (PostgreSQL keeps forever)
```

---

### **Benefits**

| Benefit | Current | With Hybrid Storage |
|---------|---------|---------------------|
| **Persistence** | âŒ Lost on restart | âœ… Permanent |
| **Speed** | âœ… Fast (~0.1ms) | âœ… Fast (~1ms Redis hit) |
| **Scalability** | âŒ Single server | âœ… Multi-server ready |
| **Analytics** | âŒ No data | âœ… Full history |
| **User Experience** | âŒ Lose history | âœ… History preserved |

---

## 2. Why Hybrid Storage?

### **Option A: PostgreSQL Only**
```
User â†’ Flask â†’ PostgreSQL
              (~10ms read)
```
**Pros**: Simple, permanent
**Cons**: Slower (10ms vs 1ms)

---

### **Option B: Redis Only**
```
User â†’ Flask â†’ Redis
              (~1ms read)
```
**Pros**: Fast
**Cons**: Data loss risk, no analytics

---

### **Option C: Hybrid (Redis + PostgreSQL)** âœ…
```
User â†’ Flask â†’ Redis (cache) â†’ PostgreSQL (source of truth)
              (~1ms hit)        (~10ms miss)
```
**Pros**: Fast + permanent + scalable
**Cons**: More complex

---

### **Why Hybrid is Best**

**Performance**: 90%+ requests hit Redis cache (~1ms)
**Reliability**: PostgreSQL is source of truth (no data loss)
**Scalability**: Multiple Flask servers share same Redis + PostgreSQL
**Analytics**: All data in PostgreSQL for analysis
**Cost-effective**: Redis only caches recent/active sessions (small memory footprint)

---

## 3. Architecture Design

### **3.1 Data Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Sends Message                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ 1. Check Redis Cache  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚
     Cache HIT              Cache MISS
          â”‚                       â”‚
          â†“                       â†“
    Return data         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    (~1ms)              â”‚ 2. Load from     â”‚
          â”‚             â”‚    PostgreSQL    â”‚
          â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â†“
          â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚             â”‚ 3. Cache in      â”‚
          â”‚             â”‚    Redis (1h TTL)â”‚
          â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â†“
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ 4. Return to User     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Receives Message                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When user sends NEW message:
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Save to PostgreSQL (permanent)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Update Redis cache            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **3.2 Cache Strategy**

**Write-Through Cache**:
```python
def save_message(conversation_id, message):
    # 1. Write to PostgreSQL (source of truth)
    db.messages.create({
        'conversation_id': conversation_id,
        'role': message['role'],
        'content': message['content']
    })

    # 2. Update Redis cache (for fast reads)
    messages = get_all_messages(conversation_id)
    redis.setex(
        f"conversation:{conversation_id}",
        3600,  # 1 hour TTL
        json.dumps(messages)
    )
```

**Benefits**:
- PostgreSQL always has latest data
- Redis cache stays fresh
- No cache invalidation complexity

---

### **3.3 Cache Eviction**

**Time-based (TTL)**:
```python
# Redis auto-deletes after 1 hour
redis.setex("conversation:123", 3600, data)
```

**Why 1 hour?**
- Active users: Likely to return within 1 hour â†’ Cache hit
- Inactive users: Cache evicted â†’ Reload from PostgreSQL
- Memory efficient: Only active sessions in Redis

**Memory calculation**:
```
Average conversation: 10 messages Ã— 500 bytes = 5KB
100 concurrent users: 100 Ã— 5KB = 500KB
1,000 concurrent users: 1,000 Ã— 5KB = 5MB
10,000 concurrent users: 10,000 Ã— 5KB = 50MB

Redis memory needed: < 100MB for most apps
```

---

## 4. Prerequisites

### **4.1 PostgreSQL Setup**

**Option A: Local Development**
```bash
# Install PostgreSQL
# Windows: Download from postgresql.org
# Mac: brew install postgresql
# Linux: sudo apt-get install postgresql

# Start PostgreSQL
# Windows: Service starts automatically
# Mac: brew services start postgresql
# Linux: sudo systemctl start postgresql

# Create database
createdb chatbot_dev
```

**Option B: Docker (Recommended)**
```yaml
# docker-compose.yml (already exists)
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: chatbot_dev
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
```

**Option C: Cloud (Production)**
- Railway: https://railway.app/ (Free tier: 500MB)
- Render: https://render.com/ (Free tier: 90 days)
- Supabase: https://supabase.com/ (Free tier: 500MB)

---

### **4.2 Redis Setup**

**Option A: Local Development**
```bash
# Install Redis
# Windows: https://github.com/microsoftarchive/redis/releases
# Mac: brew install redis
# Linux: sudo apt-get install redis-server

# Start Redis
# Windows: Run redis-server.exe
# Mac: brew services start redis
# Linux: sudo systemctl start redis
```

**Option B: Docker (Recommended)**
```yaml
# docker-compose.yml
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes  # Enable persistence
    volumes:
      - redis_data:/data
```

**Option C: Cloud (Production)**
- Upstash: https://upstash.com/ (Free tier: 10,000 commands/day)
- Redis Cloud: https://redis.com/cloud/ (Free tier: 30MB)

---

### **4.3 Install Dependencies**

**Add to `requirements.txt`**:
```
redis>=5.0.0
prisma>=0.11.0  # Already installed
```

**Install**:
```bash
pip install redis
```

---

### **4.4 Environment Variables**

**Add to `.env`**:
```bash
# PostgreSQL
DATABASE_URL="postgresql://postgres:postgres@localhost:5432/chatbot_dev"

# Redis
REDIS_URL="redis://localhost:6379/0"
REDIS_TTL=3600  # 1 hour cache TTL

# Session settings
SESSION_MAX_MESSAGES=20  # Max messages per conversation to cache
```

---

## 5. Database Schema Design

### **5.1 Core Tables**

**Users** (already exists from Prisma):
```prisma
model User {
  id         String   @id @default(uuid())
  email      String   @unique
  dept_id    String?
  created_at DateTime @default(now())

  conversations Conversation[]
}
```

**Conversations** (NEW):
```prisma
model Conversation {
  id         String   @id @default(uuid())
  user_id    String
  title      String?  // Auto-generated from first message
  created_at DateTime @default(now())
  updated_at DateTime @updatedAt

  user     User      @relation(fields: [user_id], references: [id])
  messages Message[]

  @@index([user_id])
  @@index([updated_at])
}
```

**Messages** (NEW):
```prisma
model Message {
  id              String   @id @default(uuid())
  conversation_id String
  role            String   // 'user' or 'assistant'
  content         String   @db.Text
  tokens_used     Int?
  latency_ms      Int?
  created_at      DateTime @default(now())

  conversation Conversation @relation(fields: [conversation_id], references: [id], onDelete: Cascade)

  @@index([conversation_id])
  @@index([created_at])
}
```

**QueryLogs** (NEW - for analytics):
```prisma
model QueryLog {
  id                   String   @id @default(uuid())
  user_id              String
  conversation_id      String?
  query                String   @db.Text
  response_time_ms     Int
  tokens_used          Int
  retrieval_quality    Float?   // From self-reflection
  recommendation       String?  // ANSWER, REFINE, CLARIFY, EXTERNAL
  refinement_count     Int      @default(0)
  contexts_retrieved   Int      @default(0)
  created_at           DateTime @default(now())

  @@index([user_id])
  @@index([created_at])
  @@index([recommendation])
}
```

---

### **5.2 Full Prisma Schema**

**File**: `backend/prisma/schema.prisma`

```prisma
generator client {
  provider = "prisma-client-py"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

// Existing User model (keep as is)
model User {
  id         String   @id @default(uuid())
  email      String   @unique
  password   String
  dept_id    String?
  created_at DateTime @default(now())

  conversations Conversation[]
  query_logs    QueryLog[]
}

// NEW: Conversation model
model Conversation {
  id         String   @id @default(uuid())
  user_id    String
  title      String?
  created_at DateTime @default(now())
  updated_at DateTime @updatedAt

  user     User      @relation(fields: [user_id], references: [id], onDelete: Cascade)
  messages Message[]

  @@index([user_id])
  @@index([updated_at])
}

// NEW: Message model
model Message {
  id              String   @id @default(uuid())
  conversation_id String
  role            String
  content         String   @db.Text
  tokens_used     Int?
  latency_ms      Int?
  created_at      DateTime @default(now())

  conversation Conversation @relation(fields: [conversation_id], references: [id], onDelete: Cascade)

  @@index([conversation_id])
  @@index([created_at])
}

// NEW: QueryLog model (for analytics)
model QueryLog {
  id                   String   @id @default(uuid())
  user_id              String
  conversation_id      String?
  query                String   @db.Text
  response_time_ms     Int
  tokens_used          Int
  retrieval_quality    Float?
  recommendation       String?
  refinement_count     Int      @default(0)
  contexts_retrieved   Int      @default(0)
  created_at           DateTime @default(now())

  user User @relation(fields: [user_id], references: [id], onDelete: Cascade)

  @@index([user_id])
  @@index([created_at])
  @@index([recommendation])
}
```

---

## 6. Implementation Plan

### **Phase 1: Database Setup** (1 hour)

**Step 1.1: Update Prisma Schema**
```bash
# File: backend/prisma/schema.prisma
# Add Conversation, Message, QueryLog models (see section 5.2)
```

**Step 1.2: Generate Migration**
```bash
cd backend
npx prisma migrate dev --name add_conversation_history
```

**Step 1.3: Generate Prisma Client**
```bash
npx prisma generate
```

**Step 1.4: Verify Tables Created**
```bash
npx prisma studio
# Opens UI at http://localhost:5555
# Check tables: Conversation, Message, QueryLog
```

---

### **Phase 2: Redis Connection** (30 minutes)

**Step 2.1: Create Redis Client**

**File**: `backend/src/config/redis_client.py` (NEW)

```python
"""
Redis client singleton for caching.
"""
import redis
from src.config.settings import Config

class RedisClient:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.client = redis.from_url(
                Config.REDIS_URL,
                decode_responses=True  # Auto-decode bytes to str
            )
        return cls._instance

    def get_client(self):
        return self.client

# Singleton instance
redis_client = RedisClient().get_client()
```

**Step 2.2: Add Config**

**File**: `backend/src/config/settings.py`

```python
# Add to Config class
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
REDIS_TTL = int(os.getenv("REDIS_TTL", "3600"))  # 1 hour
SESSION_MAX_MESSAGES = int(os.getenv("SESSION_MAX_MESSAGES", "20"))
```

**Step 2.3: Test Connection**

```python
# Test script
from src.config.redis_client import redis_client

# Test set/get
redis_client.set("test", "hello")
assert redis_client.get("test") == "hello"
print("âœ… Redis connection working")
```

---

### **Phase 3: Storage Service** (2 hours)

**Step 3.1: Create ConversationService**

**File**: `backend/src/services/conversation_service.py` (NEW)

**Structure**:
```python
"""
Conversation storage service with Redis caching.
"""
import json
from typing import List, Dict, Optional
from prisma import Prisma
from src.config.redis_client import redis_client
from src.config.settings import Config

class ConversationService:
    """
    Manages conversation history with hybrid storage:
    - Redis: Fast cache (1 hour TTL)
    - PostgreSQL: Permanent storage
    """

    def __init__(self, db: Prisma):
        self.db = db
        self.redis = redis_client
        self.ttl = Config.REDIS_TTL

    async def get_messages(self, conversation_id: str, limit: int = 20) -> List[Dict]:
        """
        Get conversation messages (Redis first, PostgreSQL fallback).

        Flow:
        1. Check Redis cache
        2. If hit: return cached data (~1ms)
        3. If miss: load from PostgreSQL (~10ms) â†’ cache in Redis
        """
        # TODO: Implement
        pass

    async def save_message(self, conversation_id: str, message: Dict) -> None:
        """
        Save message to PostgreSQL + update Redis cache.

        Flow:
        1. Save to PostgreSQL (source of truth)
        2. Update Redis cache (for fast reads)
        """
        # TODO: Implement
        pass

    async def create_conversation(self, user_id: str) -> str:
        """Create new conversation, return conversation_id."""
        # TODO: Implement
        pass

    async def delete_conversation(self, conversation_id: str) -> None:
        """Delete conversation from PostgreSQL + Redis."""
        # TODO: Implement
        pass

    def _cache_key(self, conversation_id: str) -> str:
        """Generate Redis cache key."""
        return f"conversation:{conversation_id}"

    async def _load_from_db(self, conversation_id: str, limit: int) -> List[Dict]:
        """Load messages from PostgreSQL."""
        # TODO: Implement
        pass

    def _cache_messages(self, conversation_id: str, messages: List[Dict]) -> None:
        """Cache messages in Redis with TTL."""
        # TODO: Implement
        pass
```

**Step 3.2: Implement Methods** (detailed implementation in code phase)

---

### **Phase 4: Integrate with Chat Route** (2 hours)

**Step 4.1: Update Chat Route**

**File**: `backend/src/routes/chat.py`

**Changes**:
```python
# Replace SESSIONS dict with ConversationService

# OLD:
SESSIONS = defaultdict(lambda: deque(...))

# NEW:
from src.services.conversation_service import ConversationService
conversation_service = ConversationService(db)

# OLD:
def get_session_history(sid, n):
    return list(SESSIONS[sid])[-n:]

# NEW:
async def get_session_history(conversation_id, n):
    return await conversation_service.get_messages(conversation_id, limit=n)

# OLD:
SESSIONS[sid].append(message)

# NEW:
await conversation_service.save_message(conversation_id, message)
```

**Step 4.2: Update Agent Route**

Same changes for `/chat/agent` endpoint.

---

### **Phase 5: Analytics & Query Logging** (1 hour)

**Step 5.1: Log Query Metrics**

**File**: `backend/src/services/agent_tools.py`

**Add logging**:
```python
# After self-reflection evaluation
await db.query_log.create({
    'user_id': context.get('user_id'),
    'conversation_id': context.get('conversation_id'),
    'query': query,
    'response_time_ms': elapsed_ms,
    'tokens_used': total_tokens,
    'retrieval_quality': eval_result.confidence,
    'recommendation': eval_result.recommendation.value,
    'refinement_count': refinement_count,
    'contexts_retrieved': len(ctx)
})
```

**Step 5.2: Create Analytics Endpoint**

**File**: `backend/src/routes/analytics.py` (NEW)

```python
@analytics_bp.get("/api/analytics/overview")
async def analytics_overview():
    """Get high-level analytics."""
    total_queries = await db.query_log.count()
    avg_quality = await db.query_log.group_by(['recommendation'])
    # ... more aggregations
    return jsonify({...})
```

---

### **Phase 6: Migration from In-Memory** (30 minutes)

**Step 6.1: Create Migration Script**

**File**: `backend/scripts/migrate_sessions.py` (NEW)

```python
"""
Migrate existing in-memory sessions to PostgreSQL.
(One-time script, run before deployment)
"""
import pickle
from prisma import Prisma
from src.routes.chat import SESSIONS

async def migrate():
    db = Prisma()
    await db.connect()

    for session_id, messages in SESSIONS.items():
        # Create conversation
        conv = await db.conversation.create({
            'id': session_id,
            'user_id': 'unknown',  # Extract from session if possible
            'title': messages[0]['content'][:50] if messages else 'Untitled'
        })

        # Create messages
        for msg in messages:
            await db.message.create({
                'conversation_id': conv.id,
                'role': msg['role'],
                'content': msg['content']
            })

    print(f"âœ… Migrated {len(SESSIONS)} sessions")

if __name__ == "__main__":
    import asyncio
    asyncio.run(migrate())
```

**Step 6.2: Run Migration**
```bash
python backend/scripts/migrate_sessions.py
```

---

## 7. Migration Strategy

### **7.1 Zero-Downtime Deployment**

**Step 1: Deploy with Both Systems** (Parallel mode)
```python
# Keep SESSIONS dict + add PostgreSQL
# Write to both, read from PostgreSQL
```

**Step 2: Verify PostgreSQL Working**
```python
# Monitor for 1 week
# Check logs, performance, errors
```

**Step 3: Remove In-Memory Dict**
```python
# Delete SESSIONS dict
# Only use PostgreSQL + Redis
```

---

### **7.2 Rollback Plan**

**If PostgreSQL fails**:
```python
# Revert to in-memory
# Restore from PostgreSQL backup
# Fix issue
# Retry deployment
```

---

## 8. Testing Strategy

### **8.1 Unit Tests**

**File**: `backend/tests/test_conversation_service.py` (NEW)

```python
import pytest
from src.services.conversation_service import ConversationService

@pytest.mark.asyncio
async def test_create_conversation():
    """Test conversation creation."""
    service = ConversationService(db)
    conv_id = await service.create_conversation("user123")
    assert conv_id is not None

@pytest.mark.asyncio
async def test_save_and_get_messages():
    """Test message storage and retrieval."""
    service = ConversationService(db)
    conv_id = await service.create_conversation("user123")

    # Save message
    await service.save_message(conv_id, {
        'role': 'user',
        'content': 'Hello'
    })

    # Get messages
    messages = await service.get_messages(conv_id)
    assert len(messages) == 1
    assert messages[0]['content'] == 'Hello'

@pytest.mark.asyncio
async def test_redis_cache_hit():
    """Test Redis cache hit performance."""
    service = ConversationService(db)
    conv_id = await service.create_conversation("user123")

    # First call: PostgreSQL (cache miss)
    import time
    start = time.time()
    await service.get_messages(conv_id)
    miss_time = time.time() - start

    # Second call: Redis (cache hit)
    start = time.time()
    await service.get_messages(conv_id)
    hit_time = time.time() - start

    # Redis should be faster
    assert hit_time < miss_time
```

---

### **8.2 Integration Tests**

**File**: `backend/tests/test_chat_integration.py`

```python
@pytest.mark.asyncio
async def test_chat_persists_history():
    """Test that chat history persists across restarts."""
    # Send message
    response = client.post("/chat", json={
        'messages': [{'role': 'user', 'content': 'Hello'}]
    })

    # Simulate Flask restart (clear in-memory cache)
    redis_client.flushall()

    # Get history (should load from PostgreSQL)
    history = await conversation_service.get_messages(conv_id)
    assert len(history) > 0
```

---

### **8.3 Performance Tests**

**File**: `backend/tests/test_performance.py`

```python
@pytest.mark.asyncio
async def test_cache_performance():
    """Benchmark Redis vs PostgreSQL performance."""
    import time

    # PostgreSQL query
    start = time.time()
    messages = await db.message.find_many(
        where={'conversation_id': conv_id}
    )
    db_time = time.time() - start

    # Redis query
    start = time.time()
    cached = redis_client.get(f"conversation:{conv_id}")
    redis_time = time.time() - start

    print(f"PostgreSQL: {db_time*1000:.2f}ms")
    print(f"Redis: {redis_time*1000:.2f}ms")

    assert redis_time < db_time  # Redis should be faster
```

---

## 9. Success Criteria

### **Must Have** âœ…

- [ ] Conversations persist across Flask restarts
- [ ] Messages saved to PostgreSQL
- [ ] Redis cache working (< 2ms reads)
- [ ] Cache hit rate > 80%
- [ ] No data loss
- [ ] Migration from in-memory complete

### **Should Have** â­

- [ ] Query logging to analytics table
- [ ] Analytics endpoint working
- [ ] Performance metrics (p50, p95, p99)
- [ ] Unit test coverage > 80%

### **Nice to Have** ğŸ¯

- [ ] Conversation title auto-generation
- [ ] Message search functionality
- [ ] User conversation list API
- [ ] Export conversation feature

---

## 10. Troubleshooting

### **Issue 1: Redis connection fails**

**Error**: `redis.exceptions.ConnectionError`

**Fix**:
```bash
# Check Redis running
redis-cli ping
# Should respond: PONG

# Check Redis URL in .env
REDIS_URL=redis://localhost:6379/0
```

---

### **Issue 2: PostgreSQL migration fails**

**Error**: `Migration failed: column already exists`

**Fix**:
```bash
# Reset database (DEV ONLY!)
npx prisma migrate reset

# Or fix migration manually
npx prisma migrate resolve --rolled-back <migration-name>
```

---

### **Issue 3: Cache not updating**

**Symptom**: Old messages showing up

**Fix**:
```python
# Force cache invalidation
redis_client.delete(f"conversation:{conversation_id}")

# Or clear all cache
redis_client.flushdb()
```

---

### **Issue 4: Slow performance**

**Symptom**: Responses taking > 100ms

**Debug**:
```python
import time

start = time.time()
messages = await service.get_messages(conv_id)
print(f"Time: {(time.time() - start) * 1000:.2f}ms")

# Check cache hit rate
info = redis_client.info('stats')
print(f"Hit rate: {info['keyspace_hits'] / (info['keyspace_hits'] + info['keyspace_misses'])}")
```

---

## 11. File Checklist

### **Files to Create**:

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ redis_client.py                # Redis singleton
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ conversation_service.py        # Hybrid storage service
â”œâ”€â”€ prisma/
â”‚   â””â”€â”€ schema.prisma                      # Updated with new models
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ migrate_sessions.py                # One-time migration
â””â”€â”€ tests/
    â”œâ”€â”€ test_conversation_service.py       # Unit tests
    â”œâ”€â”€ test_chat_integration.py           # Integration tests
    â””â”€â”€ test_performance.py                # Performance benchmarks
```

### **Files to Modify**:

```
backend/
â”œâ”€â”€ .env                                    # Add Redis config
â”œâ”€â”€ requirements.txt                        # Add redis
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py                    # Add Redis settings
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ chat.py                        # Replace SESSIONS with service
â””â”€â”€ docker-compose.yml                      # Add Redis service (optional)
```

---

## 12. Cost Analysis

### **Redis Cost** (Cloud)

**Upstash** (Recommended for small apps):
- Free tier: 10,000 commands/day
- Paid: $0.20 per 100K commands
- Estimate: $5-10/month for moderate traffic

**Redis Cloud**:
- Free tier: 30MB
- Paid: $7/month for 250MB

---

### **PostgreSQL Cost** (Cloud)

**Railway**:
- Free tier: 500MB, 100 hours/month
- Paid: $5/month for 8GB

**Render**:
- Free tier: 90 days, 1GB
- Paid: $7/month for 1GB

**Supabase**:
- Free tier: 500MB
- Paid: $25/month for 8GB + extras

---

### **Total Estimated Cost**:

**Development**: $0 (local Docker)
**Production (small)**: $10-15/month (Upstash + Railway)
**Production (medium)**: $30-50/month (Redis Cloud + Render)

---

## 13. Performance Benchmarks

### **Expected Performance**:

| Operation | Current (In-Memory) | PostgreSQL Only | Redis + PostgreSQL |
|-----------|-------------------|-----------------|-------------------|
| **Read (cache hit)** | 0.1ms | 10ms | 1ms |
| **Read (cache miss)** | 0.1ms | 10ms | 10ms (first time) |
| **Write** | 0.1ms | 15ms | 15ms (PostgreSQL) |
| **Cache hit rate** | 100% | N/A | 80-95% |

### **Throughput**:

- In-memory: 10,000 req/sec
- PostgreSQL only: 500 req/sec
- Redis + PostgreSQL: 5,000 req/sec (with 80% cache hit rate)

---

## 14. Summary

### **What You'll Get**:

**Before** (In-Memory):
- âŒ Lost on restart
- âŒ Single server only
- âŒ No analytics
- âŒ No user history

**After** (Redis + PostgreSQL):
- âœ… Permanent storage
- âœ… Multi-server ready
- âœ… Full analytics
- âœ… User history preserved
- âœ… Fast performance (1ms cache hits)
- âœ… Scalable to millions of users

**Time to implement**: 6-8 hours
**Value added**: Production-ready persistence + performance
**Resume impact**: Shows full-stack + infrastructure skills

---

**Ready to implement? Review this guide, then we'll proceed with code changes!** ğŸš€
